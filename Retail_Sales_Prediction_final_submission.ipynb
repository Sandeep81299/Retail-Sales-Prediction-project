{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "g-ATYxFrGrvw",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sandeep81299/Retail-Sales-Prediction-project/blob/main/Retail_Sales_Prediction_final_submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -**Retails Sales Prediction** \n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1** -**Sandeep Bhagwat Salunke**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Retail Sales prediction data is provided with two csv files that is Rossman and Stores which contains   different variables of information.It consists combinly about 19 variables which contain different kind of information\n",
        "\n",
        "As the first step of the project we have performed data cleaning as well as data wrangling by merging both of tables in the next step we  have performed Exploratory  Data Analysis(EDA) in which we created different visualization charts to analyze the data we found  that some interesting facts like sales are highly correlated with customers,there were more sales on Monday,probably because shops are closed on Sundays,it could be seen that the promo leads to more sales,more stores open on school holidays than on state holiday and hence had more sales than state holidays,on an average store type B had the highest sales,Highest average sales were seen with Assortment levels-b which is ‘extra’,82.1% sales are not affected and only 17.9% sales is affected because of school holiday etc.\n",
        "\n",
        "In the next step I have done hypothetical testing Hypothesis: Stores located closer to competition have significantly lower sales than stores located further away.Null hypothesis: There is no significant difference in sales between stores located closer to competition and stores located further away.\n",
        "Alternative hypothesis: Stores located closer to competition have significantly lower sales than stores located further away.To test this hypothesis, we can perform a two-sample t-test between the sales of stores located within 10 km of competition and\n",
        "stores located further away. We can set a significance level of 0.05\n",
        "\n",
        "After that I have performed feature engineering  like filling missing values,handling of null values,handling columns,deleting unnecessary columns,feature processing,feature extracting,Outliers handling,feature selection.\n",
        "\n",
        "In the last step most important step of my project that is model deployment. I have deployed two models first one is the linear regression and second one is the lasso regression final conclusion of both of the models -The MSE and R2 score are commonly used evaluation metrics for regression models. In this case, the Linear Regression and Lasso Regression models have very similar performance, with the Lasso Regression model having a slightly lower MSE and a slightly higher R2 score.The mean squared error (MSE) measures the average squared difference between the predicted and actual values, where a lower MSE indicates better performance.The Rsquared (R2) score measures the proportion of the variance in the dependent variable that is predictable from the independent variables, where a higher R2 score indicates better performance."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Sandeep81299/Retail-Sales-Prediction-project."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Ross*mann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied.**\n",
        "\n",
        " **You are provided with historical sales data for 1,115 Rossmann stores. The task is to forecast the \"Sales\" column for the test set. Note that some stores in the dataset were temporarily closed for refurbishment.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "import missingno as msno\n",
        "import matplotlib\n",
        "import matplotlib.pylab as pylab\n",
        "\n",
        "%matplotlib inline\n",
        "matplotlib.style.use('ggplot')\n",
        "sns.set_style('white')\n",
        "pylab.rcParams['figure.figsize'] = 8,6\n",
        "\n",
        "import math\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LassoLars\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import ElasticNet"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu1kxsG-cW7U"
      },
      "outputs": [],
      "source": [
        "#Loading Rossman Dataset\n",
        "rossman_df= pd.read_csv('/content/drive/MyDrive/Rossmann Stores Data (2).csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading Store Dataset\n",
        "stores_df=pd.read_csv('/content/drive/MyDrive/store (1).csv', low_memory= False)"
      ],
      "metadata": {
        "id": "CcgwgZAHVJjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rossman Dataset**"
      ],
      "metadata": {
        "id": "DlD1BBDsVt6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#First Look\n",
        "rossman_df.head(10)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFuI8hegek4a"
      },
      "outputs": [],
      "source": [
        "rossman_df.tail(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Store Dataset**"
      ],
      "metadata": {
        "id": "ZicB_NT0V5FZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stores_df.head(10)"
      ],
      "metadata": {
        "id": "f-v7kJxyV-xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stores_df.tail(10)"
      ],
      "metadata": {
        "id": "YT_P2JsCWEex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count of rossman dataset\n",
        "rossman_df.shape\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count of store dataset\n",
        "stores_df.shape"
      ],
      "metadata": {
        "id": "eCMzGBCNWYo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Merging of both datasets**"
      ],
      "metadata": {
        "id": "7lcy6IuHWySW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#merge the datasets on stores data\n",
        "df = rossman_df.merge(right=stores_df, on=\"Store\", how=\"left\")"
      ],
      "metadata": {
        "id": "plor1OczW8Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#first ten rows of the merged dataset\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "8MSR6uSAXGsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "Bz0oqjZSYjhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "# Find duplicate rows based on all columns\n",
        "duplicate_rows = df[df.duplicated()]\n",
        "\n",
        "# Print the number of duplicate rows\n",
        "print(f'There are {len(duplicate_rows)} duplicate rows in the data.')\n",
        "\n",
        "# Find duplicate rows based on a specific column(s)\n",
        "duplicate_sales = df[df.duplicated(subset=['Sales'])]\n",
        "\n",
        "# Print the number of duplicate sales\n",
        "print(f'There are {len(duplicate_sales)} rows with duplicate sales values.')"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# creating heatmap for null values\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(stores_df.isnull(),yticklabels= False, cbar= False, cmap= 'viridis')"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#distribution plot of competition distance\n",
        "sns.distplot(x=df['CompetitionDistance'], hist = True)\n",
        "plt.xlabel('Competition Distance Distribution Plot')"
      ],
      "metadata": {
        "id": "t6vk3oCxj_lD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data including two datasets rossman and store.Their is 1017209 rows and 9 columns in rossman datset and 1115 rows 9 columns in store dataset after merging the both of datasets 1017209 rows 18 columns this two of columns contains 0 duplicated rows and columns but contains duplicated values.Store dataset containing the missing values"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b><u>Data fields</u></b>\n",
        "### Most of the fields are self-explanatory. The following are descriptions for those that aren't.\n",
        "\n",
        "* #### Id - an Id that represents a (Store, Date) duple within the test set\n",
        "* #### Store - a unique Id for each store\n",
        "* #### Sales - the turnover for any given day (this is what you are predicting)\n",
        "* #### Customers - the number of customers on a given day\n",
        "* #### Open - an indicator for whether the store was open: 0 = closed, 1 = open\n",
        "* #### StateHoliday - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None\n",
        "* #### SchoolHoliday - indicates if the (Store, Date) was affected by the closure of public schools\n",
        "* #### StoreType - differentiates between 4 different store models: a, b, c, d\n",
        "* #### Assortment - describes an assortment level: a = basic, b = extra, c = extended\n",
        "* #### CompetitionDistance - distance in meters to the nearest competitor store\n",
        "* #### CompetitionOpenSince[Month/Year] - gives the approximate year and month of the time the nearest competitor was opened\n",
        "* #### Promo - indicates whether a store is running a promo on that day\n",
        "* #### Promo2 - Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\n",
        "* #### Promo2Since[Year/Week] - describes the year and calendar week when the store started participating in Promo2\n",
        "* #### PromoInterval - describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for column in df.columns:\n",
        "    unique_values = df[column].unique()\n",
        "    print(f'{column}: {unique_values}')"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI4OLuetsoGF"
      },
      "source": [
        "#### **Replace missing values in features with low percentages of missing values**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CompetitionDistance is distance in meters to the nearest competitor store\n",
        "# let's first have a look at its distribution\n",
        "\n",
        "sns.distplot(df.CompetitionDistance.dropna())\n",
        "plt.title(\"Distributin of Store Competition Distance\")"
      ],
      "metadata": {
        "id": "bImRdBfkHxe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace missing values in CompetitionDistance with median for the store dataset\n",
        "\n",
        "df.CompetitionDistance.fillna(df.CompetitionDistance.median(), inplace=True)"
      ],
      "metadata": {
        "id": "hoBgjjWXH8W2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a categorical column list \n",
        "categorical_variables = ['DayOfWeek','Open','Promo','StateHoliday','SchoolHoliday','StoreType','Assortment','CompetitionOpenSinceMonth',\n",
        "                         'CompetitionOpenSinceYear','Promo2','Promo2SinceWeek','Promo2SinceYear','PromoInterval']"
      ],
      "metadata": {
        "id": "3rmqNwbBYE1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating features from the date\n",
        "df['Year'] = pd.DatetimeIndex(df['Date']).year\n",
        "df['Month'] = pd.DatetimeIndex(df['Date']).month\n",
        "df['WeekOfYear'] = pd.DatetimeIndex(df['Date']).week\n",
        "df['DayOfYear'] = pd.DatetimeIndex(df['Date']).dayofyear\n",
        "years = df['Year'].unique()"
      ],
      "metadata": {
        "id": "GoWe3Hw6cq-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  * Replace missing values in features with low percentages of missing values\n",
        "  * replace missing values in CompetitionDistance with median for the store dataset\n",
        "  * creating a categorical column list it is also necessary to work with categorical columns\n",
        "  * We have changed stateholiday to integer type\n",
        "  * creating features from the date"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization \n",
        "#code for barplots of the categorical variables against sales\n",
        "#iterating over the categorical variables and plotting each of them\n",
        "categorical_variables = ['DayOfWeek','Open','Promo','StateHoliday','SchoolHoliday','StoreType','Assortment','CompetitionOpenSinceMonth',\n",
        "                         'Promo2','Promo2SinceYear','PromoInterval']\n",
        "for value in categorical_variables:\n",
        "  ax = sns.barplot(x=df[value], y=df['Sales']) \n",
        "  totals = []\n",
        "  for i in ax.patches: #for every patch in the barplot ax\n",
        "      totals.append(i.get_height()) #append height for each patch\n",
        "\n",
        "  total = sum(totals) #sum of each patch height for a plot\n",
        "\n",
        "  for i in ax.patches:  \n",
        "      ax.text(i.get_x() - .01, i.get_height() + .5, \\\n",
        "              str(round((i.get_height()/total)*100, 2))+'%', fontsize=12)  #text position and formula for percentage\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I want to all insights about the categorical values"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Observation:\n",
        "*  There were more sales on Monday, probably because shops generally remain closed on Sundays.\n",
        "* It could be seen that the Promo leads to more sales.\n",
        "* Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None. Lowest of Sales were seen on state holidays especially on Christmas.\n",
        "* More stores were open on School Holidays than on State Holidays and hence had more sales than State Holidays.\n",
        "* On an average Store type B had the highest sales.\n",
        "* Highest average sales were seen with Assortment levels-b which is 'extra'.\n",
        "* With Promo2, slightly more sales were seen without it which indicates there are many stores not participating in promo.\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "> Yes it is important to know what impact do categorical values on sales.Yes their is some negative insights such as less sale on state holiday and if promo not used their is aslo less sales.\n",
        "\n"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "#Sales affected on schoolholidays or not\n",
        "labels = ['Not-Affected', 'Affected']\n",
        "sizes = rossman_df.SchoolHoliday.value_counts()\n",
        "colors = ['purple', 'red']\n",
        "\n",
        "# Plot the pie chart\n",
        "fig, ax = plt.subplots()\n",
        "ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%',\n",
        "       shadow=True, startangle=180, explode=(0.1, 0.0), \n",
        "       wedgeprops={\"edgecolor\":\"0\",'linewidth': 1,'linestyle': 'solid', 'antialiased': True})\n",
        "ax.set_title(\"Sales Affected by Schoolholiday or Not ?\", fontsize=20)\n",
        "ax.axis('equal')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know the how many sales afffected by schoolholiday."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* 82.1% sales are not affected and  only 17.9% sales is affected because of schoo holiday"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes definitely because we will able to know to improve on each areas.It is positive signs that the most of sales is not affected only some % affected"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "sns.lmplot(x='Customers', y='Sales', data=df)"
      ],
      "metadata": {
        "id": "NKdq7VUPg3a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " To know the customer vs sales relation."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* As we can see their is linear relationship between customers and sales as customers increasing sales also increasing "
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "it is positive signs that customers increasing then sales also increasing"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "#Promo vs sales relation\n",
        "sns.factorplot(data = df, x =\"Month\", y = \"Sales\", \n",
        "               col = 'Promo', # per store type in cols\n",
        "               hue = 'Promo2',\n",
        "               row = \"Year\")"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know the promo vs sales relation"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Here we can see that if their is no promo the sales is very less and if promo running their the sales is high."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes it creates very much impact because we can observe that if promo using their is sales also increasing and if promo not using it is negative impact on sales"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "#Barchart of promo vs sales impact\n",
        "sns.barplot(data = df, x = \"DayOfWeek\", y = \"Sales\", hue = \"Promo\")"
      ],
      "metadata": {
        "id": "0f0DkDdlIM4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know the exact diffrence of sales when promo using and not using"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Their is large diffrence on monday and it is decreasing day by day and on sunday their is no sales so it shwing less."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* NO, it is actually not we can say because of promo only sale is increasing we should focus on increasing sales without promo aslo or we should use promo everyday"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "#Trend of sales over the years\n",
        "sns.factorplot(data = df, x = \"Month\", y = \"Sales\", col = \"Year\", hue = \"StoreType\")"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know the trend of sales over the years."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?\n"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In 2013 and 2014 their is some increasing in the sales but in 2015 their is some decreasing in trend of sales over the months"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It showing some negative growth in last year it should be some improve"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization \n",
        "#competition distance stores sales\n",
        "sns.scatterplot(x=df['CompetitionDistance'], y=df['Sales'])"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know the competition stores distance and sales"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above scatter plot it can be observed that mostly the competitor stores weren't that far from each other and the stores densely located near each other saw more sales"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes we are able to know the competitor stores and their sales make strategies according to it."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Sum of sales by store type\n",
        "sales_by_store_type = df.groupby(\"StoreType\")[\"Sales\"].sum()\n",
        "\n",
        "# Plotting pie chart of sales by store type\n",
        "fig, ax = plt.subplots()\n",
        "ax.pie(sales_by_store_type, labels=sales_by_store_type.index, autopct='%1.1f%%', shadow=True)\n",
        "ax.set_title('Store Type and Sales')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Sum of customers by store type\n",
        "customers_by_store_type = df.groupby(\"StoreType\")[\"Customers\"].sum()\n",
        "\n",
        "# Plotting pie chart of customers by store type\n",
        "fig, ax = plt.subplots()\n",
        "ax.pie(customers_by_store_type, labels=customers_by_store_type.index, autopct='%1.1f%%', shadow=True)\n",
        "ax.set_title('Customer Share')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Count of store types in the dataset\n",
        "store_types = df[\"StoreType\"].value_counts()\n",
        "\n",
        "# Plotting pie chart of store types in the dataset\n",
        "fig, ax = plt.subplots()\n",
        "ax.pie(store_types, labels=store_types.index, autopct='%1.1f%%', shadow=True)\n",
        "ax.set_title('Share of Store Types')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know the Storetype and Sales,Customer Share,Share of Storetype"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A bar plot represents an estimate of central tendency for a numeric variable with the height of each rectangle. Earlier it was seen that the store type b had the highest sales on an average because the default estimation function to the barplot is mean. \n",
        "* But upon further exploration it can be clearly observed that the highest sales belonged to the store type a due to the high number of type a stores in our dataset. Store type a and c had a similar kind of sales and customer share.\n",
        "* Interesting insight to note is that store type b with highest average sales and per store revenue generation looks healthy and a reason for that would be all three kinds of assortment strategies involved which was seen earlier."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Storetype A and B doing good sales but it is worry about c and d sales"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "#we need only meaningful numeric columns here, let's drop the unnecessary to get a clear picture\n",
        "columns_to_drop = ['Store', 'Year', 'WeekOfYear', 'DayOfYear']\n",
        "corr_df = df.drop(columns = columns_to_drop, axis =1)\n",
        "corr_df['StateHoliday'].replace({'a':1, 'b':1,'c':1}, inplace=True)\n",
        "#correlation heatmap\n",
        "plt.figure(figsize=(16,10))\n",
        "sns.heatmap(corr_df.corr(), cmap=\"YlGnBu\", annot=True)"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know the correlations about categorical values."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Observation:\n",
        "* Day of the week has a negative correlation indicating low sales as the weekends, and promo, customers and open has positive correlation.\n",
        "* State Holiday has a negative correlation suggesting that stores are mostly closed on state holidays indicating low sales.\n",
        "* CompetitionDistance showing negative correlation suggests that as the distance increases sales reduce, which was also observed through the scatterplot earlier.\n",
        "* There's multicollinearity involved in the dataset as well. The features telling the same story like Promo2, Promo2 since week and year are showing multicollinearity.\n",
        "* The correlation matrix is agreeing with all the observations done earlier while exploring through barplots and scatterplots.\n"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis: Stores located closer to competition have significantly lower sales than stores located further away.\n",
        "\n",
        "Null hypothesis: There is no significant difference in sales between stores located closer to competition and stores located further away.\n",
        "\n",
        "Alternative hypothesis: Stores located closer to competition have significantly lower sales than stores located further away.\n",
        "\n",
        "To test this hypothesis, we can perform a two-sample t-test between the sales of stores located within 10 kms of competition and stores located further away. We can set a significance level of 0.05.\n",
        "\n"
      ],
      "metadata": {
        "id": "urR2dMRX5bMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the p-value is less than the significance level (0.05), we can reject the null hypothesis and conclude that stores located closer to competition have significantly lower sales than stores located further away. Otherwise, we fail to reject the null hypothesis and conclude that there is no significant difference in sales between the two groups."
      ],
      "metadata": {
        "id": "zWXAiAY_649_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Remove features with high percentages of missing values**\n",
        "\n",
        "#### **we can see that some features have a high percentage of missing values and they won't be accurate as indicators, so we will remove features with more than 30% missing values.**"
      ],
      "metadata": {
        "id": "qF7VvMJZJC57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df =df.drop(['CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear','Promo2SinceWeek',\n",
        "                     'Promo2SinceYear', 'PromoInterval'], axis=1)"
      ],
      "metadata": {
        "id": "C9rXUliZBQ_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CompetitionDistance is distance in meters to the nearest competitor store\n",
        "# let's first have a look at its distribution\n",
        "\n",
        "sns.distplot(df.CompetitionDistance.dropna())\n",
        "plt.title(\"Distributin of Store Competition Distance\")"
      ],
      "metadata": {
        "id": "oOLNZSjbJarJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **The distribution is right skewed, so we'll replace missing values with the median.**"
      ],
      "metadata": {
        "id": "SdoS4CymJf2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# replace missing values in CompetitionDistance with median for the store dataset\n",
        "\n",
        "df.CompetitionDistance.fillna(df.CompetitionDistance.median(), inplace=True)"
      ],
      "metadata": {
        "id": "orvvCvAlJibk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outliers Handling"
      ],
      "metadata": {
        "id": "UF7XgEFtxak4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#removing outliers\n",
        "def remove_outlier(df_in, col_name):\n",
        "    q1 = df_in[col_name].quantile(0.25)\n",
        "    q3 = df_in[col_name].quantile(0.75)\n",
        "    iqr = q3-q1 #Interquartile range\n",
        "    fence_low  = q1-1.5*iqr\n",
        "    fence_high = q3+1.5*iqr\n",
        "    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n",
        "    return df_out"
      ],
      "metadata": {
        "id": "1H5VH6PQss5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining new variable after removing outliers\n",
        "df= remove_outlier(df, 'Sales')"
      ],
      "metadata": {
        "id": "Ev1mbqR4s0Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "RKYkA0Zo2Xgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "provided is using the Interquartile Range (IQR) method to remove outliers from a specified column of a pandas DataFrame."
      ],
      "metadata": {
        "id": "-VM8v6e3yB6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "#no of observations for closed stores with 0 sales\n",
        "# where stores are closed, they won't generate sales, so we will remove that part of the dataset\n",
        "df = df[df.Open != 0]"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "######It is mentioned in the problem statement that some stores were temporarily closed for refurbishment and hence did not generate any sales. This was also indicated in the barplot of Open vs Sales."
      ],
      "metadata": {
        "id": "iig5n1FwYhhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open isn't a variable anymore, so we'll drop it too\n",
        "df = df.drop('Open', axis=1)"
      ],
      "metadata": {
        "id": "LsOLJgUXYkHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXGh57FQwBsL"
      },
      "source": [
        "# Check if there's any opened store with zero sales\n",
        "df[df.Sales == 0]['Store'].sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PiThQJmwVfm"
      },
      "source": [
        "# see the percentage of open stored with zero sales\n",
        "df[df.Sales == 0]['Sales'].sum()/df.Sales.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove this part of data to avoid bias\n",
        "df = df[df.Sales != 0]"
      ],
      "metadata": {
        "id": "z_X4CUPOrhpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new=df.copy()"
      ],
      "metadata": {
        "id": "KIZHH6uyroA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__KwbiepsXq4"
      },
      "source": [
        "df_new = pd.get_dummies(df_new,columns=['StoreType','Assortment'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.head()"
      ],
      "metadata": {
        "id": "_z7yfoodr0hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot for sales in terms of days ofthe week\n",
        "plt.figure(figsize=(15,8))\n",
        "sns.barplot(x='DayOfWeek', y='Sales' ,data=df_new); "
      ],
      "metadata": {
        "id": "UZpLPIiYr8x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_new.drop(['Sales','Store','Date','Year','StateHoliday'] , axis = 1)\n",
        "y= df_new.Sales"
      ],
      "metadata": {
        "id": "jpJIVjifsWGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "IaOxdBEmsYPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a method called \"train-test split\" to split the dataset into input features and target variable.\n",
        "\n",
        "In this method, the dataset is split into two subsets: one for training the machine learning model and the other for testing the model's performance. The X variable contains the input features that will be used to train the model, and the y variable contains the corresponding target variable.\n",
        "\n",
        "This method is commonly used in machine learning to evaluate the performance of the model on unseen data. By splitting the data into training and testing sets, we can assess how well the model will generalize to new data that it has not seen before."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific features that are included in X depend on the problem domain and the availability of data. In this case, the features that have been included are likely to be important indicators of sales, as determined by the analyst or domain expert who created the dataset.\n",
        "\n",
        "By selecting relevant features, we can improve the accuracy of the machine learning model's predictions. Using irrelevant or redundant features can actually decrease the model's performance and lead to overfitting.\n",
        "\n",
        "Therefore, it is important to carefully select the features that are used as input to a machine learning model, as they can have a significant impact on the model's performance and ability to make accurate predictions."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data points with sales value higher than 10.2 are very low and hence they an be considered as outliers. The percentage of outliers in our dataset:"
      ],
      "metadata": {
        "id": "iiqeP4nLhqJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.head(5)"
      ],
      "metadata": {
        "id": "Zka5fMqiyeKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.info()"
      ],
      "metadata": {
        "id": "3QGs2ncwVAXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.head"
      ],
      "metadata": {
        "id": "-7zdDCXXykjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Splitting Dataset Into Training Set and Test Set**"
      ],
      "metadata": {
        "id": "ZIcw7HigtG0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state=0)"
      ],
      "metadata": {
        "id": "msvdhweYtJjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns=X_train.columns"
      ],
      "metadata": {
        "id": "y5PG9FahtW9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - **1.Linear Regression**"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# fit the linear regression model\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# make predictions on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# evaluate the model\n",
        "mse = mean_squared_error(y_test[:120], y_pred[:120])\n",
        "rmse = np.sqrt(mse)\n",
        "train_score = regressor.score(X_train[:120], y_train[:120])\n",
        "test_score = regressor.score(X_test[:120], y_test[:120])\n",
        "print(\"Train Score: \", train_score)\n",
        "print(\"Test Score: \", test_score)\n",
        "print(\"RMSE: \", rmse)\n",
        "\n",
        "# plot the predicted values against the real values using Seaborn\n",
        "sns.set_style('whitegrid')\n",
        "sns.lmplot(x='y_test', y='y_pred', data=pd.DataFrame({'y_test': y_test[:100], 'y_pred': y_pred[:100]}), aspect=1.5, scatter_kws={'alpha':0.2})\n",
        "plt.xlabel('Real Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Linear Regression Model')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "acYHNDVqEqP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "reg = LinearRegression().fit(X, y)\n",
        "\n",
        "# print model coefficients and intercept\n",
        "print('Coefficients:', reg.coef_)\n",
        "print('Intercept:', reg.intercept_)\n",
        "\n",
        "# print R-squared score on training set\n",
        "print('Training R-squared:', reg.score(X, y))"
      ],
      "metadata": {
        "id": "jRf9HahXGq1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The linear regression model is a simple and commonly used algorithm for predicting numerical values based on a set of input features.\n",
        "\n",
        "The model's performance is evaluated using the R-squared score, which measures the proportion of the variance in the target variable that can be explained by the model. In this case, the R-squared score on the training set is 0.7812, which indicates that the model explains 78.12% of the variance in the target variable on the training set.\n",
        "\n",
        " However, the test score of 0.7816 suggests that the model's performance on the test set is similar to its performance on the training set.\n",
        "\n",
        "In addition to the R-squared score, the code also calculates the root mean squared error (RMSE) on the test set, which measures the average difference between the predicted and actual values. The RMSE value of 1174.0436 suggests that the model's predictions are, on average, about 1174 units away from the actual values.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-agSXJLWZBMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# create a linear regression model\n",
        "regressor = LinearRegression()\n",
        "\n",
        "# define the parameter grid for hyperparameter tuning\n",
        "param_grid = {'fit_intercept': [True, False], 'normalize': [True, False]}\n",
        "\n",
        "# create a GridSearchCV object with 5-fold cross-validation\n",
        "grid = GridSearchCV(regressor, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# fit the model with training data\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# get the best estimator\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "# evaluate the model on training and test data\n",
        "train_score = best_model.score(X_train, y_train)\n",
        "test_score = best_model.score(X_test, y_test)\n",
        "\n",
        "# make predictions on test data\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# calculate RMSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# calculate cross-validation scores\n",
        "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "cv_rmse = np.sqrt(-cv_scores)\n",
        "\n",
        "# print the evaluation metrics\n",
        "print('Best Model:', best_model)\n",
        "print('Train Score:', train_score)\n",
        "print('Test Score:', test_score)\n",
        "print('Train RMSE:', rmse)\n",
        "print('CV RMSE:', cv_rmse.mean())\n"
      ],
      "metadata": {
        "id": "tKtxQmbl4RJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####1. Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, I have used GridSearchCV, which is a technique for hyperparameter optimization. I used it because it exhaustively searches over the specified parameter values for an estimator and finds the best combination of hyperparameters that gives the highest cross-validation score. It also uses cross-validation to evaluate the performance of the model with each hyperparameter combination, which helps in avoiding overfitting and in generalizing well to new data."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####2. Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there was an improvement in the model's performance after applying cross-validation and hyperparameter tuning. The new evaluation metrics are:\n",
        "\n",
        "Train Score: 0.7807369723730564\n",
        "Test Score: 0.7823704821647295\n",
        "Train RMSE: 1153.0557010195887\n",
        "CV RMSE: 1155.0742700578603\n",
        "Compared to the original model, the train score decreased slightly, but the test score increased. The RMSE score also decreased, indicating improved predictive accuracy. Overall, the model's performance improved after applying cross-validation and hyperparameter tuning."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "V-sFcVkQ8ZJ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Score and Test Score: The train score of 0.788 and test score of 0.782 suggest that the model is performing well in terms of accuracy, with a slightly higher score on the training data compared to the test data. In terms of business impact, this means that the model can provide reasonably accurate predictions for the target variable, which can be valuable for making decisions such as forecasting sales, estimating customer lifetime value, or predicting demand.\n",
        "\n",
        "RMSE: The root mean squared error (RMSE) of 1174.04 suggests that, on average, the model's predictions are off by approximately 1174 units. In terms of business impact, this metric can be used to evaluate the accuracy of the model's predictions, and to identify areas where the model may need improvement. For example, if the model is being used to predict customer lifetime value, a high RMSE may indicate that the model is not accurately predicting the true lifetime value of customers, which could impact decisions related to marketing or customer acquisition.\n",
        "\n",
        "Train RMSE and CV RMSE: The train RMSE of 1153.02 and CV RMSE of 1155.08 suggest that the model is performing well in terms of accuracy, with a slightly lower RMSE on the training data compared to the cross-validated data. In terms of business impact, this means that the model can provide reasonably accurate predictions for the target variable, which can be valuable for making decisions such as forecasting sales, estimating customer lifetime value, or predicting demand. Additionally, the fact that the training and cross-validated RMSE scores are similar suggests that the model is not overfitting to the training data, which is important for ensuring that the model can generalize well to new, unseen data."
      ],
      "metadata": {
        "id": "tnA6zm5ng-Ct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - **2.Lasso Regression**"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state=0)"
      ],
      "metadata": {
        "id": "o_TL0eKTuHnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns=X_train.columns"
      ],
      "metadata": {
        "id": "90TTGdPQuORx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "las = LassoLars(alpha=0.3, fit_intercept=False, normalize=True)\n",
        "lasreg = las.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "HCYFqi4f1zCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse(x, y):\n",
        "    return np.sqrt(mean_squared_error(x, y))\n",
        "\n",
        "# define MAPE function\n",
        "def mape(x, y):\n",
        "    return np.mean(np.abs((x - y) / x)) * 100\n",
        "\n",
        "train_score_2 = lasreg.score(X_train[:120], y_train[:120])\n",
        "test_score_2 = lasreg.score(X_test[:120], y_test[:120])\n",
        "\n",
        "y_predicted = lasreg.predict(X_train[:120])\n",
        "y_test_predicted = lasreg.predict(X_test[:120])\n",
        "\n",
        "print(\"Regression Model Score:\", train_score_2, \",\", \"Out of Sample Test Score:\", test_score_2)\n",
        "print(\"Training RMSE:\", rmse(y_train[:120], y_predicted), \"Testing RMSE:\", rmse(y_test[:120], y_test_predicted))\n",
        "print(\"Training MAPE:\", mape(y_train[:120], y_predicted), \"Testing MAPE:\", mape(y_test[:120], y_test_predicted))\n"
      ],
      "metadata": {
        "id": "8ta6ZWC_133N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The given ML model is a lasso regression model that uses some features to predict a target variable. The training score of the model is 0.788, indicating that it is performing well on the training data. The out-of-sample test score of the model is 0.782, indicating that the model generalizes well to new data.\n",
        "\n",
        "The Root Mean Squared Error (RMSE) of the training data is 1194.56, which means that the model's predictions are, on average, 1194.56 units away from the actual value. The RMSE of the test data is 1172.61, which is slightly better than the training data, indicating that the model is not overfitting the training data.\n",
        "\n",
        "The Mean Absolute Percentage Error (MAPE) is a relative error metric that measures the percentage difference between the predicted and actual values. The MAPE of the training data is 15.03%, indicating that, on average, the model's predictions are 15.03% away from the actual values. The MAPE of the test data is 15.60%, which is slightly higher than the training data, indicating that the model may be slightly overfitting to the training data.\n",
        "\n",
        "Overall, the model seems to be performing well on both the training and test data, with reasonably low errors and high scores."
      ],
      "metadata": {
        "id": "y5nS1u_Y2ZM8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define RMSE and MAPE functions\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "# Load and split the data into train and test sets # replace this with your data loading code\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Lasso regression model\n",
        "lasso = Lasso()\n",
        "\n",
        "# Create a pipeline with a scaler and the Lasso model\n",
        "pipeline = make_pipeline(StandardScaler(), lasso)\n",
        "\n",
        "# Define a grid of hyperparameters to search over\n",
        "param_grid = {\n",
        "    'lasso__alpha': [0.1, 1.0, 10.0, 100.0],\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object with 5-fold cross-validation\n",
        "grid = GridSearchCV(pipeline, param_grid, cv=5, scoring=make_scorer(rmse))\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and the corresponding RMSE score\n",
        "print(\"Best hyperparameters:\", grid.best_params_)\n",
        "print(\"RMSE score:\", grid.best_score_)\n",
        "\n",
        "# Use the best model to predict on the test set\n",
        "y_test_predicted = grid.predict(X_test)\n",
        "\n",
        "# Print the test RMSE and MAPE scores\n",
        "print(\"Test RMSE:\", rmse(y_test, y_test_predicted))\n",
        "print(\"Test MAPE:\", mape(y_test, y_test_predicted))\n"
      ],
      "metadata": {
        "id": "__jal7gZJo5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####1. Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used GridSearchCV for hyperparameter optimization. I chose this technique because it exhaustively searches over a specified parameter grid, which ensures that the best combination of hyperparameters is found. Additionally, it uses cross-validation to evaluate the model's performance, which gives a more accurate estimate of how well the model will generalize to new data."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2.Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scores suggest that the model is performing fairly well For example, a regression model with an R-squared score of 0.788 is explaining 78.8% of the variance in the dependent variable, which could be considered a good fit in some situations. However, it might not be sufficient for other cases where higher accuracy is required.\n",
        "\n",
        "Similarly, the training and testing RMSE scores suggest that the model's predictions are, on average, off by approximately 1194 and 1172 units, respectively, which may be acceptable in some applications but not in others.\n",
        "\n",
        "Regarding the cross-validation results, the best hyperparameter value suggests that the Lasso regularization with an alpha value of 100 provides the best fit for the model.\n",
        "\n",
        "Overall, the evaluation metrics provided appear reasonable, but their suitability depends on the specific context of the problem at hand."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n"
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression Model Score (R-squared): The R-squared value of 0.788 suggests that the model is able to explain approximately 79% of the variance in the dependent variable using the input features. In terms of business impact, this means that the model is able to provide a good fit to the data, and can be used to make accurate predictions about the target variable. For example, if the model is being used to predict sales, a high R-squared value would indicate that the model is able to explain a large proportion of the variability in sales, and can be used to make more accurate sales forecasts.\n",
        "\n",
        "Out of Sample Test Score: The out-of-sample test score of 0.782 indicates that the model is able to generalize well to new, unseen data. In terms of business impact, this means that the model is likely to perform well when making predictions on new data, which is important for ensuring that the model can be used to make accurate predictions in real-world scenarios.\n",
        "\n",
        "Training RMSE and Testing RMSE: The root mean squared error (RMSE) measures the average difference between the actual and predicted values of the target variable. The training RMSE of 1194.56 and testing RMSE of 1172.61 suggest that the model's predictions are, on average, off by approximately 1194 and 1172 units, respectively. In terms of business impact, these metrics can be used to evaluate the accuracy of the model's predictions, and to identify areas where the model may need improvement. For example, if the model is being used to predict customer lifetime value, a high RMSE may indicate that the model is not accurately predicting the true lifetime value of customers, which could impact decisions related to marketing or customer acquisition.\n",
        "\n",
        "Training MAPE and Testing MAPE: The mean absolute percentage error (MAPE) measures the average difference between the actual and predicted values of the target variable as a percentage of the actual value. The training MAPE of 15.03 and testing MAPE of 15.60 suggest that the model's predictions are, on average, off by approximately 15% of the actual value. In terms of business impact, these metrics can be used to evaluate the accuracy of the model's predictions in a more interpretable way than RMSE, and can be used to compare the accuracy of different models. For example, if the model is being used to predict demand for a product, a high MAPE may indicate that the model is not accurately predicting the true level of demand, which could impact decisions related to production and inventory management.."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Regression**."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_score_1=0.788097798119638"
      ],
      "metadata": {
        "id": "2TC9Z4D9sMnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_score_1=0.782385691617763"
      ],
      "metadata": {
        "id": "mm2Y3gWhsfQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lasso Regression**"
      ],
      "metadata": {
        "id": "693bqwSys5mO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_score_2=0.7880849733619616"
      ],
      "metadata": {
        "id": "6g_7_vr5sfZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_score_2=0.7821477157945707"
      ],
      "metadata": {
        "id": "QgPpC72MsfhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_df = pd.DataFrame({'Train_Score': [train_score_1, train_score_2], 'Test_Score': [test_score_1, test_score_2]}, index=['Linear Regression', 'Lasso Regression'])\n",
        "score_df"
      ],
      "metadata": {
        "id": "wa1L4nf6kHt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Train Linear Regression model\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train, y_train)\n",
        "\n",
        "# Train Lasso Regression model\n",
        "lasso_reg = Lasso(alpha=0.1)\n",
        "lasso_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test set\n",
        "lin_reg_pred = lin_reg.predict(X_test)\n",
        "lasso_reg_pred = lasso_reg.predict(X_test)\n",
        "\n",
        "# Compute performance metrics for both models\n",
        "lin_reg_mse = mean_squared_error(y_test, lin_reg_pred)\n",
        "lasso_reg_mse = mean_squared_error(y_test, lasso_reg_pred)\n",
        "\n",
        "lin_reg_r2 = r2_score(y_test, lin_reg_pred)\n",
        "lasso_reg_r2 = r2_score(y_test, lasso_reg_pred)\n",
        "\n",
        "# Print performance metrics\n",
        "print(\"Linear Regression MSE: \", lin_reg_mse)\n",
        "print(\"Lasso Regression MSE: \", lasso_reg_mse)\n",
        "\n",
        "print(\"Linear Regression R2 score: \", lin_reg_r2)\n",
        "print(\"Lasso Regression R2 score: \", lasso_reg_r2)\n"
      ],
      "metadata": {
        "id": "kK5VPDskwiiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MSE and R2 score are commonly used evaluation metrics for regression models. In this case, the Linear Regression and Lasso Regression models have very similar performance, with the Lasso Regression model having a slightly lower MSE and a slightly higher R2 score.\n",
        "\n",
        "The mean squared error (MSE) measures the average squared difference between the predicted and actual values, where a lower MSE indicates better performance. The R-squared (R2) score measures the proportion of the variance in the dependent variable that is predictable from the independent variables, where a higher R2 score indicates better performance."
      ],
      "metadata": {
        "id": "dCwBhSq0wv1C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}